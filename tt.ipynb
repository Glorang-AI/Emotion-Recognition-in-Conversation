{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.15) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "/home/jjonhwa/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import wandb\n",
    "import argparse\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import AdamW, Wav2Vec2Config, RobertaConfig, BertConfig, AutoTokenizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from dataset import ETRIDataset\n",
    "from trainer import ModelTrainer\n",
    "from models import CASEmodel, RoCASEmodel\n",
    "from utils import audio_embedding, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "from typing import Callable, Tuple, Dict\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class ETRIDataset(Dataset):\n",
    "    \"\"\"\n",
    "    This is a class that returns audio embeddings and text tokenization results.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 audio_embedding, # audio embedding load from '~.pt' file\n",
    "                 dataset:pd.DataFrame, \n",
    "                 label_dict:Dict, # label to idx dictionary\n",
    "                 tokenizer:Callable,\n",
    "                 audio_emb_type:str='last_hidden_state', # audio embedding type: 'last_hidden_state' or 'extract_features'\n",
    "                 max_len:int=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.audio_emb = audio_embedding\n",
    "        self.dataset = dataset\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_dict = label_dict\n",
    "        self.emb_type = audio_emb_type\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.dataset['text'].iloc[idx]\n",
    "        \n",
    "        label = torch.tensor(self.label_dict[self.dataset['labels'].iloc[idx]])\n",
    "        \n",
    "        emb_key = str(self.dataset.index[idx])\n",
    "        wav_emb = self.audio_emb[emb_key][self.emb_type]\n",
    "\n",
    "        encoded_dict = self.tokenizer(text, \n",
    "                                      return_tensors='pt',\n",
    "                                      add_special_tokens=True,\n",
    "                                      max_length=self.max_len,\n",
    "                                      padding='max_length',\n",
    "                                      truncation=True,\n",
    "                                      return_attention_mask=True,\n",
    "                                      return_token_type_ids=True\n",
    "                                      )\n",
    "        \n",
    "        encoded_dict['audio_emb'] = wav_emb\n",
    "        encoded_dict['label'] = label\n",
    "        \n",
    "        return encoded_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "# -- Choose Pretrained Model\n",
    "parser.add_argument(\"--lm_path\", type=str, default=\"klue/bert-base\", help=\"You can choose models among (klue-bert series and klue-roberta series) (default: klue/bert-base\")\n",
    "parser.add_argument(\"--am_path\", type=str, default=\"kresnik/wav2vec2-large-xlsr-korean\")\n",
    "\n",
    "# -- Training Argument\n",
    "parser.add_argument(\"--lr\", type=float, default=1e-5)\n",
    "parser.add_argument(\"--train_bsz\", type=int, default=64)\n",
    "parser.add_argument(\"--valid_bsz\", type=int, default=256)\n",
    "parser.add_argument(\"--val_ratio\", type=float, default=0.2)\n",
    "parser.add_argument(\"--context_max_len\", type=int, default=128)\n",
    "parser.add_argument(\"--audio_max_len\", type=int, default=1024)\n",
    "parser.add_argument(\"--epochs\", type=int, default=20)\n",
    "parser.add_argument(\"--num_labels\", type=int, default=7)\n",
    "parser.add_argument(\"--audio_emb_type\", type=str, default=\"last_hidden_state\", help=\"Can chosse audio embedding type between 'last_hidden_state' and 'extract_features' (default: last_hidden_state)\")\n",
    "parser.add_argument(\"--model\", type=str, default=\"CASE\")\n",
    "## -- directory\n",
    "parser.add_argument(\"--data_path\", type=str, default=\"data/train.csv\")\n",
    "parser.add_argument(\"--save_path\", type=str, default=\"save\")\n",
    "###### emb_train에 대한 설명 부과하기\n",
    "parser.add_argument(\"--embedding_path\", type=str, default=\"data/emb_train.pt\")\n",
    "\n",
    "# -- utils\n",
    "parser.add_argument(\"--device\", type=str, default=\"cuda:0\")\n",
    "parser.add_argument(\"--num_workers\", type=int, default=4)\n",
    "parser.add_argument(\"--seed\", type=int, default=0)\n",
    "\n",
    "# -- wandb\n",
    "parser.add_argument(\"--wandb_project\", type=str, default=\"comp\")\n",
    "parser.add_argument(\"--wandb_entity\", type=str, default=None)\n",
    "parser.add_argument(\"--wandb_group\", type=str, default=None)\n",
    "parser.add_argument(\"--wandb_name\", type=str, default=\"case_audio_base\")\n",
    "\n",
    "args = parser.parse_args([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_config = Wav2Vec2Config.from_pretrained(args.am_path)\n",
    "bert_config = BertConfig.from_pretrained(args.lm_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.lm_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_audio_collator(batch):\n",
    "    \n",
    "    return {'audio_emb' : pad_sequence([item['audio_emb'] for item in batch], batch_first=True),\n",
    "            'label' : torch.stack([item['label'] for item in batch]).squeeze(),\n",
    "            'input_ids' :  torch.stack([item['input_ids'] for item in batch]).squeeze(),\n",
    "            'attention_mask' :  torch.stack([item['attention_mask'] for item in batch]).squeeze(),\n",
    "            'token_type_ids' :  torch.stack([item['token_type_ids'] for item in batch]).squeeze()}\n",
    "\n",
    "dataset = pd.read_csv(args.data_path)\n",
    "dataset.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(dataset, test_size = args.val_ratio, random_state=args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kresnik/wav2vec2-large-xlsr-korean were not used when initializing Wav2Vec2Model: ['lm_head.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing Wav2Vec2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 10262/10262 [04:29<00:00, 38.13it/s]\n"
     ]
    }
   ],
   "source": [
    "audio_emb = audio_embedding.save_and_load(args.am_path, dataset['audio'].tolist(), args.device, args.embedding_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_dict = {'angry':0, 'neutral':1, 'sad':2, 'happy':3, 'disqust':4, 'surprise':5, 'fear':6}\n",
    "train_dataset = ETRIDataset(\n",
    "    audio_embedding = audio_emb, \n",
    "    dataset=train_df, \n",
    "    label_dict = label_dict,\n",
    "    tokenizer = tokenizer,\n",
    "    audio_emb_type = args.audio_emb_type,\n",
    "    max_len = args.context_max_len, \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=args.train_bsz,\n",
    "        shuffle=True, \n",
    "        collate_fn=text_audio_collator, \n",
    "        num_workers=args.num_workers,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8209/8209 [00:01<00:00, 7167.01it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "for i in tqdm(range(len(train_dataset))):\n",
    "    train_dataset[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp = next(iter(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'audio_emb', 'label'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samp.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_761833/2472082722.py\", line 41, in __getitem__\n    wav_emb = self.audio_emb[emb_key][self.emb_type]\nKeyError: '1392'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sample \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39miter\u001b[39;49m(train_dataloader))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1333\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1331\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1332\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1333\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:1359\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1358\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1359\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1360\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_utils.py:543\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    540\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    541\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    542\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 543\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mKeyError\u001b[0m: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/home/jjonhwa/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\", line 58, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_761833/2472082722.py\", line 41, in __getitem__\n    wav_emb = self.audio_emb[emb_key][self.emb_type]\nKeyError: '1392'\n"
     ]
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
