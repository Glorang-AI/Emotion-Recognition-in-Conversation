Some weights of the model checkpoint at klue/bert-base were not used when initializing BertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias']
- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
  0%|                                                                                                                | 0/20 [00:00<?, ?it/s]


























100%|█████████████████████████████████████████████████████████████████████████████████| 145/145 [00:52<00:00,  3.46it/s, loss=4.95, lr=1e-5]
  0%|                                                                                                                | 0/20 [00:56<?, ?it/s]
Traceback (most recent call last):
  File "main.py", line 108, in <module>
    main()
  File "main.py", line 105, in main
    trainer.train()
  File "/home/popul/work_dir/Emotion-Recognition-in-Conversation/trainer.py", line 29, in train
    val_loss = self._validation()
  File "/home/popul/work_dir/Emotion-Recognition-in-Conversation/trainer.py", line 101, in _validation
    m_f1 = self._macro_f1_score(output_list, label_list)
  File "/home/popul/work_dir/Emotion-Recognition-in-Conversation/trainer.py", line 116, in _macro_f1_score
    m_f1_score = multiclass_f1_score(logits, labels,
  File "/home/popul/work_dir/venv/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/home/popul/work_dir/venv/lib/python3.8/site-packages/torcheval/metrics/functional/classification/f1_score.py", line 112, in multiclass_f1_score
    num_tp, num_label, num_prediction = _f1_score_update(
  File "/home/popul/work_dir/venv/lib/python3.8/site-packages/torcheval/metrics/functional/classification/f1_score.py", line 160, in _f1_score_update
    _f1_score_update_input_check(input, target, num_classes)
  File "/home/popul/work_dir/venv/lib/python3.8/site-packages/torcheval/metrics/functional/classification/f1_score.py", line 268, in _f1_score_update_input_check
    raise ValueError(
ValueError: input should have shape of (num_sample,) or (num_sample, num_classes), got torch.Size([1027, 768]).