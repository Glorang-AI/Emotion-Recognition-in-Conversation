{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 70\u001b[0m\n\u001b[1;32m     58\u001b[0m audio_emb \u001b[39m=\u001b[39m audio_embedding\u001b[39m.\u001b[39msave_and_load(config[\u001b[39m'\u001b[39m\u001b[39mam_path\u001b[39m\u001b[39m'\u001b[39m], dataset[\u001b[39m'\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto_list(),\n\u001b[1;32m     59\u001b[0m                                                 \u001b[39m'\u001b[39m\u001b[39mcuda:3\u001b[39m\u001b[39m'\u001b[39m,  \u001b[39m# cuda is required to run the audio embedding generation model.\u001b[39;00m\n\u001b[1;32m     60\u001b[0m                                                 config[\u001b[39m'\u001b[39m\u001b[39membedding_path\u001b[39m\u001b[39m'\u001b[39m]) \n\u001b[1;32m     62\u001b[0m train_dataset \u001b[39m=\u001b[39m ETRIDataset(audio_embedding \u001b[39m=\u001b[39m audio_emb, \n\u001b[1;32m     63\u001b[0m                                 dataset\u001b[39m=\u001b[39mtrain_df, \n\u001b[1;32m     64\u001b[0m                                 label_dict \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39m\u001b[39mlabel_dict\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                 max_len \u001b[39m=\u001b[39m wandb\u001b[39m.\u001b[39mconfig[\u001b[39m'\u001b[39m\u001b[39mmax_len\u001b[39m\u001b[39m'\u001b[39m], \n\u001b[1;32m     68\u001b[0m                                 )\n\u001b[0;32m---> 70\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m train_dataset:\n\u001b[1;32m     71\u001b[0m     i\n",
      "File \u001b[0;32m~/work_dir/Emotion-Recognition-in-Conversation/dataset.py:34\u001b[0m, in \u001b[0;36mETRIDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 34\u001b[0m     text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39m'\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m'\u001b[39;49m][idx]\n\u001b[1;32m     35\u001b[0m     label \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabel_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[\u001b[39m'\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m'\u001b[39m][idx]]])\n\u001b[1;32m     36\u001b[0m     wav_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maudio_emb[idx][\u001b[39mself\u001b[39m\u001b[39m.\u001b[39memb_type]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[1;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[1;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[1;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[1;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from dataset import ETRIDataset\n",
    "from trainer import ModelTrainer\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import audio_embedding, seed\n",
    "from models import CASEmodel, RoCASEmodel\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AdamW, Wav2Vec2Config, RobertaConfig, BertConfig, AutoTokenizer\n",
    "\n",
    "# Define a config dictionary object\n",
    "config = {\n",
    "  \"lr\": 1e-5,\n",
    "  \"lm_path\":'klue/bert-base',\n",
    "  \"am_path\": 'kresnik/wav2vec2-large-xlsr-korean',\n",
    "  \"train_bsz\": 64,\n",
    "  \"val_bsz\": 64,\n",
    "  \"val_ratio\":0.1,\n",
    "  \"max_len\": 128,\n",
    "  \"epochs\" :20,\n",
    "  \"device\":'cuda:2',\n",
    "  \"num_labels\":7,\n",
    "  'data_path' : 'data/train.csv',\n",
    "  \"label_dict\": {'angry':0, 'neutral':1, 'sad':2, 'happy':3, 'disqust':4, 'surprise':5, 'fear':6},\n",
    "  \"sav_dir\":'save',\n",
    "  \"base_score\":0.45, # Save the model according to the base validation score.\n",
    "  'embedding_path':'data/emb_train.pt', # If an embedding file named \"data/emb_train.pt\" does not exist, generate one\n",
    "  \"audio_emb_type\": 'last_hidden_state', # audio embedding type: 'last_hidden_state' or 'extract_features'\n",
    "  \"max_len\" : 128,\n",
    "  \"seed\":42\n",
    "}\n",
    "\n",
    "\n",
    "seed.seed_setting(config['seed'])\n",
    "\n",
    "# # Pass the config dictionary when you initialize W&B\n",
    "# wandb.init(project='comp',\n",
    "#         group='bert_cls',\n",
    "#         name='case_audio_base',\n",
    "#         config=config\n",
    "# )\n",
    "\n",
    "wav_config = Wav2Vec2Config.from_pretrained(config['am_path'])\n",
    "bert_config = BertConfig.from_pretrained(config['lm_path'])\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['lm_path'])\n",
    "\n",
    "def text_audio_collator(batch):\n",
    "    audio_emb = pad_sequence([item['audio_emb'] for item in batch], batch_first=True)\n",
    "    batch['audio_emb'] = audio_emb\n",
    "    return batch\n",
    "\n",
    "dataset = pd.read_csv(config['data_path'])\n",
    "dataset.reset_index(inplace=True)\n",
    "train_df, val_df = train_test_split(dataset, test_size = config['val_ratio'], random_state=config['seed'])\n",
    "\n",
    "audio_emb = audio_embedding.save_and_load(config['am_path'], dataset['audio'].to_list(),\n",
    "                                                'cuda:3',  # cuda is required to run the audio embedding generation model.\n",
    "                                                config['embedding_path']) \n",
    "\n",
    "train_dataset = ETRIDataset(audio_embedding = audio_emb, \n",
    "                                dataset=train_df, \n",
    "                                label_dict = config['label_dict'],\n",
    "                                tokenizer = tokenizer,\n",
    "                                audio_emb_type = config['audio_emb_type'],\n",
    "                                max_len = wandb.config['max_len'], \n",
    "                                )\n",
    "\n",
    "for i in train_dataset:\n",
    "    i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'그래서 이제 그래가지고 그런 것들이 몇 개 있었는데 그중에 하나가 감자랑 뭐 케찹, 특히 케찹.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.index[213]\n",
    "train_df['text'].iloc[230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "text = train_df['text'].iloc[idx]\n",
    "# label = torch.tensor([label_dict[dataset['labels'][idx]]])\n",
    "label = train_df['labels'].iloc[idx]\n",
    "emb_key = str(train_df.index[idx])\n",
    "wav_emb = audio_emb[emb_key]['last_hidden_state']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'last_hidden_state': tensor([[-0.2264,  0.4745,  0.3608,  ..., -0.6388,  0.1840, -1.1324],\n",
       "         [-0.3262,  0.3578,  0.2932,  ..., -0.2424,  0.4000, -1.1593],\n",
       "         [-0.2379, -0.0612,  0.1637,  ..., -0.6568,  1.0239, -1.2081],\n",
       "         ...,\n",
       "         [-0.2023,  0.5404,  1.0275,  ..., -1.2963,  0.4411, -1.1646],\n",
       "         [-0.0138,  0.8220,  1.2323,  ..., -1.3627, -0.1051, -0.9415],\n",
       "         [ 0.4475,  1.1410, -0.1176,  ..., -1.5894,  0.6625, -0.7491]]),\n",
       " 'extract_features': tensor([[ 0.0915, -0.9580, -0.8608,  ...,  0.1364, -0.1297, -0.5361],\n",
       "         [ 0.1730, -0.5584,  0.6368,  ..., -0.3559, -0.5224,  0.1257],\n",
       "         [ 0.4155, -0.4394,  1.1773,  ...,  0.9019,  0.1899,  0.1428],\n",
       "         ...,\n",
       "         [ 0.3458, -0.8104, -1.1789,  ...,  0.1491, -0.1316, -0.9596],\n",
       "         [ 0.3236, -0.9368, -1.0070,  ...,  0.2956,  0.0048, -0.6200],\n",
       "         [ 1.0934, -0.9369, -1.3964,  ...,  0.5943,  0.2064, -1.2340]])}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n",
      "wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n"
     ]
    }
   ],
   "source": [
    "audio_emb['5087']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
